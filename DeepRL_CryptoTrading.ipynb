{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1WU_lWwMygL4OsJT1ro90dpRpesT3pdu1","timestamp":1674789615825},{"file_id":"1wWWJAWggxvcRm6Bh7jaLKWhZeT7jobal","timestamp":1666963022101}],"machine_shape":"hm","authorship_tag":"ABX9TyNh7U9wGafUIpLT9OmWY5de"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install empyrical"],"metadata":{"id":"vpGXY2OPVyJj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import logging\n","import time\n","import numpy as np\n","import pandas as pd\n","import pandas\n","from empyrical import sharpe_ratio\n","from matplotlib import pyplot as plt\n","from turtle import pd\n","import argparse\n","import importlib\n","import sys\n","import time\n","from pathlib import Path\n","from datetime import datetime\n","import random\n","from collections import deque\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","#------------------------------------------------------------------------\n","def RSI(data, n=14):\n","    price = pandas.DataFrame(data)\n","    delta = price.diff()\n","    delta[0][0]=delta[0][1]\n","    dUp, dDown = delta.copy(), delta.copy()\n","    dUp[dUp < 0] = 0\n","    dDown[dDown > 0] = 0\n","    RolUp=dUp.ewm(span=n).mean()\n","    RolDown=dDown.ewm(span=n).mean().abs()\n","    RS = RolUp / RolDown\n","    rsi= 100.0 - (100.0 / (1.0 + RS))\n","    t_rsi=np.array(rsi.replace(np.nan,0)).tolist()\n","    #make2list(t_rsi)\n","    #print('rsi:',[l[0] for l in t_rsi])\n","    return [l[0] for l in t_rsi]\n","\n","def MACD(data):\n","    price=pd.DataFrame(data)\n","    exp1 = price.ewm(span=12, adjust=False).mean()\n","    exp2 = price.ewm(span=26, adjust=False).mean()\n","    macd = exp1-exp2\n","    exp3 = macd.ewm(span=9, adjust=False).mean()\n","    macds=macd-exp3\n","    t_macd=np.array(macds)\n","    return [l[0] for l in t_macd]\n","\n","def SMA(data, window=20):\n","    sma = data.rolling(window = window).mean()\n","    return sma\n","\n","def BB(data):\n","    #https://medium.com/codex/algorithmic-trading-with-bollinger-bands-in-python-1b0a00c9ef99\n","    price=pd.DataFrame(data)\n","    sma=SMA(price,20)\n","    std = price.rolling(window = 20).std()\n","    upper_bb = sma + std * 2\n","    lower_bb = sma - std * 2\n","    bb=(price-lower_bb)/(upper_bb-lower_bb)\n","    t_bb=np.array(bb)\n","    return [l[0] for l in t_bb]\n","\n","def OBV(data1,data2):\n","    #https://medium.com/wwblog/implement-the-on-balance-volume-obv-indicator-in-python-10ac889efe72\n","    price=pd.DataFrame(data1)\n","    volume=pd.DataFrame(data2)\n","    obv= (np.sign(price.diff()) * volume).fillna(0).cumsum()\n","    t_obv=np.array(obv)\n","    return [l[0] for l in t_obv]\n","#----------------------------------------------------------------------------------------------\n","\n","def log_txt(file,str):\n","  with open(file, 'a') as fid: # 'w' creates a new file\n","    now = datetime.now()\n","    strs = now.strftime(\"%d/%m/%Y %H:%M:%S\")+ \"\\t\" + str + \"\\n\"\n","    fid.write(strs)              \n","    #fid.writelines(['He', 'Ne', 'Ar'])  # writelines writes each element on its own\n","\n","#////////////////////////////////////////////////////////////////////////////////////////////////\n","class Portfolio:\n","    def __init__(self, balance=50000):\n","        self.initial_portfolio_value = balance\n","        self.balance = balance\n","        self.inventory = []\n","        self.return_rates = []\n","        self.portfolio_values = [balance]\n","        self.buy_dates = []\n","        self.sell_dates = []\n","\n","    def reset_portfolio(self):\n","        self.balance = self.initial_portfolio_value\n","        self.inventory = []\n","        self.return_rates = []\n","        self.portfolio_values = [self.initial_portfolio_value]\n","\n","        \n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","\n","def softmax(x):\n","    return np.exp(x) / np.sum(np.exp(x))\n","\n","\n","def stock_close_prices(key):\n","    '''return a list containing stock close prices from a .csv file'''\n","    prices = []\n","    lines = open(key + \".csv\", \"r\").read().splitlines()\n","    for line in lines[1:]:\n","        prices.append(float(line.split(\",\")[4]))\n","    return prices\n","\n","\n","def generate_price_state(stock_prices, end_index, window_size,indicator=\"\"):\n","    '''\n","    return a state representation, defined as\n","    the adjacent stock price differences after sigmoid function (for the past window_size days up to end_date)\n","    note that a state has length window_size, a period has length window_size+1\n","    '''\n","    start_index = end_index - window_size\n","    if start_index >= 0:\n","        period = stock_prices[start_index:end_index+1]\n","        if indicator==\"\":\n","            return sigmoid(np.diff(period))\n","        \n","        elif indicator==\"RSI\":\n","            rsi=RSI(period)    \n","            rsi.pop(0)            \n","            rsi_t=np.array(rsi)\n","            t_rsi=sigmoid(rsi_t/max(rsi_t))\n","            #print('rsi_indicator: ',t_rsi,len(t_rsi), type(t_rsi))\n","            return t_rsi\n","        \n","        elif indicator==\"MACD\":\n","            macd=MACD(period)\n","            if max(macd)==0:\n","                macd_indicator=sigmoid(0)\n","            else:\n","                macd_indicator=macd[-1]\n","            \n","            return macd_indicator\n","\n","    else: # if end_index cannot suffice window_size, pad with prices on start_index\n","        period = -start_index * [stock_prices[0]] + stock_prices[0:end_index+1]\n","        return sigmoid(np.diff(period))\n","\n","\n","def generate_portfolio_state(stock_price, balance, num_holding):\n","    '''logarithmic values of stock price, portfolio balance, and number of holding stocks'''\n","    return [np.log(stock_price), np.log(balance), np.log(num_holding + 1e-6)]\n","\n","\n","def generate_combined_state(end_index, window_size, stock_prices, balance, num_holding,indicator=\"\"):\n","    '''\n","    return a state representation, defined as\n","    adjacent stock prices differences after sigmoid function (for the past window_size days up to end_date) plus\n","    logarithmic values of stock price at end_date, portfolio balance, and number of holding stocks\n","    '''\n","    prince_state = generate_price_state(stock_prices, end_index, window_size,indicator)\n","    portfolio_state = generate_portfolio_state(stock_prices[end_index], balance, num_holding)\n","    print('prince_state len:',len(prince_state),'portfolio_state len :', len(portfolio_state))\n","    return np.array([np.concatenate((prince_state, portfolio_state), axis=None)])\n","\n","\n","def treasury_bond_daily_return_rate():\n","    r_year = 2.75 / 100  # approximate annual U.S. Treasury bond return rate\n","    return (1 + r_year)**(1 / 365) - 1\n","\n","\n","def maximum_drawdown(portfolio_values):\n","    end_index = np.argmax(np.maximum.accumulate(portfolio_values) - portfolio_values)\n","    if end_index == 0:\n","        return 0\n","    beginning_iudex = np.argmax(portfolio_values[:end_index])\n","    return (portfolio_values[end_index] - portfolio_values[beginning_iudex]) / portfolio_values[beginning_iudex]\n","\n","\n","def evaluate_portfolio_performance(agent, filename):\n","    portfolio_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n","    return portfolio_return\n","\n","\n","def plot_portfolio_transaction_history(stock_name, agent):\n","\tportfolio_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n","\tdf = pd.read_csv('{}.csv'.format(stock_name))\n","\tbuy_prices = [df.iloc[t, 4] for t in agent.buy_dates]\n","\tsell_prices = [df.iloc[t, 4] for t in agent.sell_dates]\n","\tplt.figure(figsize=(15, 5), dpi=100)\n","\tplt.title('{} Total Return on {}: ${:.2f}'.format(agent.model_type, stock_name, portfolio_return))\n","\tplt.plot(df['Date'], df['Close'], color='black', label=stock_name)\n","\tplt.scatter(agent.buy_dates, buy_prices, c='green', alpha=0.5, label='buy')\n","\tplt.scatter(agent.sell_dates, sell_prices,c='red', alpha=0.5, label='sell')\n","\tplt.xticks(np.linspace(0, len(df), 10))\n","\tplt.ylabel('Price')\n","\tplt.legend()\n","\tplt.grid()\n","\tplt.show()\n","\n","\n","def buy_and_hold_benchmark(stock_name, agent):\n","    df = pd.read_csv('{}.csv'.format(stock_name))\n","    dates = df['time']\n","    num_holding = agent.initial_portfolio_value // df.iloc[0, 4]\n","    balance_left = agent.initial_portfolio_value % df.iloc[0, 4]\n","    buy_and_hold_portfolio_values = df['close']*num_holding + balance_left\n","    buy_and_hold_return = buy_and_hold_portfolio_values.iloc[-1] - agent.initial_portfolio_value\n","    return dates, buy_and_hold_portfolio_values, buy_and_hold_return\n","\n","\n","def plot_portfolio_performance_comparison(stock_name, agent):\n","\tdates, buy_and_hold_portfolio_values, buy_and_hold_return = buy_and_hold_benchmark(stock_name, agent)\n","\tagent_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n","\tplt.figure(figsize=(15, 5), dpi=100)\n","\tplt.title('{} vs. Buy and Hold'.format(agent.model_type))\n","\tplt.plot(dates, agent.portfolio_values, color='green', label='{} Total Return: ${:.2f}'.format(agent.model_type, agent_return))\n","\tplt.plot(dates, buy_and_hold_portfolio_values, color='blue', label='{} Buy and Hold Total Return: ${:.2f}'.format(stock_name, buy_and_hold_return))\n","\tplt.xticks(np.linspace(0, len(dates), 10))\n","\tplt.ylabel('Portfolio Value ($)')\n","\tplt.legend()\n","\tplt.grid()\n","\tplt.show()\n","\n","def plot_all(stock_name, agent):\n","    '''combined plots of plot_portfolio_transaction_history and plot_portfolio_performance_comparison'''\n","    fig, ax = plt.subplots(2, 1, figsize=(16,8), dpi=100)\n","\n","    portfolio_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n","    df = pd.read_csv('{}.csv'.format(stock_name))\n","    buy_prices = [df.iloc[t, 4] for t in agent.buy_dates]\n","    sell_prices = [df.iloc[t, 4] for t in agent.sell_dates]\n","    ax[0].set_title('{} Total Return on {}: ${:.2f} |'.format(agent.model_type, stock_name.split('_')[0], portfolio_return))\n","    ax[0].plot(df['time'], df['close'], color='black', label=stock_name.split('_')[0])\n","    ax[0].scatter(agent.buy_dates, buy_prices, c='green', alpha=0.5, label='buy')\n","    ax[0].scatter(agent.sell_dates, sell_prices,c='red', alpha=0.5, label='sell')\n","    ax[0].set_ylabel('Price')\n","    ax[0].set_xticks(np.linspace(0, len(df), 10))\n","    ax[0].legend()\n","    ax[0].grid()\n","\n","    dates, buy_and_hold_portfolio_values, buy_and_hold_return = buy_and_hold_benchmark(stock_name, agent)\n","    agent_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n","    ax[1].set_title('{} vs. Buy and Hold'.format(agent.model_type))\n","    ax[1].plot(dates, agent.portfolio_values, color='green', label='{} Total Return: ${:.2f}'.format(agent.model_type, agent_return))\n","    ax[1].plot(dates, buy_and_hold_portfolio_values, color='blue', label='{} Buy and Hold Total Return: ${:.2f}'.format(stock_name.split('_')[0], buy_and_hold_return))\n","    \n","    ax[1].set_ylabel('Portfolio Value ($)')    \n","    ax[1].set_xticks(np.linspace(0, len(df), 10))\n","    ax[1].legend()\n","    ax[1].grid()\n","\n","    plt.subplots_adjust(hspace=0.5)\n","    plt.show()\n","\n","\n","def plot_portfolio_returns_across_episodes(model_name, returns_across_episodes,coinname,inducator):\n","    len_episodes = len(returns_across_episodes)\n","    print('returns_across_episodes:',returns_across_episodes)\n","    plt.figure(figsize=(15, 5), dpi=100)\n","    plt.title(coinname +': Portfolio Returns')\n","    plt.plot(np.arange(1, len_episodes+1),returns_across_episodes, color='black')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Return Value')\n","    plt.grid()\n","    plt.savefig('./{}_{}_{}_returns_ep{}.png'.format(coinname,model_name, inducator, len_episodes))\n","    #plt.show()\n","    plt.show(block=False)\n","    plt.pause(10)\n","    plt.close()\n","#////////////////////////////////////////////////////////////////////////////////////////////////\n","\n","\n","\n","#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n","class Agent(Portfolio):\n","    def __init__(self, state_dim, balance, is_eval=False, model_name=\"\"):\n","        super().__init__(balance=balance)\n","        self.model_type = 'RNN'\n","        self.state_dim = state_dim\n","        self.action_dim = 3  # hold, buy, sell\n","        self.memory = deque(maxlen=100)\n","        self.buffer_size = 60\n","\n","        self.gamma = 0.95\n","        self.epsilon = 1.0  # initial exploration rate\n","        self.epsilon_min = 0.01  # minimum exploration rate\n","        self.epsilon_decay = 0.995 # decrease exploration rate as the agent becomes good at trading\n","        self.is_eval = is_eval        self.model = tf.keras.models.load_model('RNN_ep10.h5') if is_eval else self.model()\n","\n","        self.tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs/RNN_tensorboard', update_freq=90)\n","        self.tensorboard.set_model(self.model)\n","\n","    def model(self):\n","        model = tf.keras.models.Sequential()\n","        model.add(tf.keras.layers.Reshape((self.state_dim,1), input_shape=(self.state_dim,1)))\n","        model.add(tf.keras.layers.SimpleRNN(32, return_sequences=True))\n","        model.add(tf.keras.layers.SimpleRNN(16, return_sequences=False))\n","        model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n","        model.add(tf.keras.layers.Dense(units=8, activation='relu'))\n","        model.add(tf.keras.layers.Dense(self.action_dim, activation='sigmoid'))\n","        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=0.01))\n","        model.summary()\n","        return model\n","\n","    def reset(self):\n","        self.reset_portfolio()\n","        self.epsilon = 1.0 # reset exploration rate\n","\n","    def remember(self, state, actions, reward, next_state, done):\n","        self.memory.append((state, actions, reward, next_state, done))\n","\n","    def act(self, state):\n","        if not self.is_eval and np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_dim)\n","        options = self.model.predict(state)\n","        return np.argmax(options[0])\n","\n","    def experience_replay(self):\n","        # retrieve recent buffer_size long memory\n","        mini_batch = [self.memory[i] for i in range(len(self.memory) - self.buffer_size + 1, len(self.memory))]\n","\n","        for state, actions, reward, next_state, done in mini_batch:\n","            if not done:\n","                Q_target_value = reward + self.gamma * np.amax(self.model.predict(next_state.reshape(1, next_state.shape[1], 1))[0])\n","            else:\n","                Q_target_value = reward\n","            next_actions = self.model.predict(state.reshape(1, state.shape[1], 1))\n","            next_actions[0][np.argmax(actions)] = Q_target_value\n","            history = self.model.fit(state.reshape(1, state.shape[1], 1), next_actions, epochs=1, verbose=0)\n","\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","        return history.history['loss'][0]\n","#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n","\n","model_name = 'RNN'\n","stock_name = 'ETHUSDT_1h'\n","window_size = 30\n","num_episode = 10\n","initial_balance = 50000\n","indicator=\"RSI\"   # \"\", RSI, MACD BB, OBV\n","\n","stock_prices = stock_close_prices(stock_name)\n","trading_period = len(stock_prices) - 1\n","returns_across_episodes = []\n","num_experience_replay = 0\n","action_dict = {0: 'Hold', 1: 'Buy', 2: 'Sell'}\n","\n","filename=f'{model_name}_{indicator}_training_{stock_name}.txt'\n","\n","log_txt(filename,f'Trading Object:           {stock_name}')\n","log_txt(filename,f'Window Size:              {window_size} step')\n","log_txt(filename,f'Training Episode:         {num_episode}')\n","log_txt(filename,f'Model Name:               {model_name}')\n","log_txt(filename,'Initial Portfolio Value: ${:,}'.format(initial_balance))\n","\n","agent = Agent(state_dim=window_size + 3, balance=initial_balance)\n","\n","\n","def hold(actions):\n","    # encourage selling for profit and liquidity\n","    next_probable_action = np.argsort(actions)[1]\n","    if next_probable_action == 2 and len(agent.inventory) > 0:\n","        max_profit = stock_prices[t] - min(agent.inventory)\n","        if max_profit > 0:\n","            sell(t)\n","            actions[next_probable_action] = 1 # reset this action's value to the highest\n","            return 'Hold', actions\n","\n","def buy(t):\n","    if agent.balance > stock_prices[t]:\n","        agent.balance -= stock_prices[t]\n","        agent.inventory.append(stock_prices[t])\n","        return 'Buy: ${:.2f}'.format(stock_prices[t])\n","\n","def sell(t):\n","    if len(agent.inventory) > 0:\n","        agent.balance += stock_prices[t]\n","        bought_price = agent.inventory.pop(0)\n","        profit = stock_prices[t] - bought_price\n","        global reward\n","        reward = profit\n","        return 'Sell: ${:.2f} | Profit: ${:.2f}'.format(stock_prices[t], profit)\n","\n","\n","start_time = time.time()\n","for e in range(1, num_episode + 1):\n","    print(f'\\nEpisode: {e}/{num_episode}')\n","    #logging.info(f'\\nEpisode: {e}/{num_episode}')\n","    log_txt(filename,f'\\nEpisode: {e}/{num_episode}')\n","\n","    agent.reset() # reset to initial balance and hyperparameters\n","    state = generate_combined_state(0, window_size, stock_prices, agent.balance, len(agent.inventory),indicator)\n","\n","    for t in range(1, trading_period + 1):\n","        if t % 100 == 0:\n","            print(f'\\n-------------------Period: {t}/{trading_period}-------------------')\n","            #logging.info(f'\\n-------------------Period: {t}/{trading_period}-------------------')\n","            log_txt(filename,f'\\n-------------------Period: {t}/{trading_period}-------------------')\n","\n","        reward = 0\n","        next_state = generate_combined_state(t, window_size, stock_prices, agent.balance, len(agent.inventory),indicator)\n","        #previous_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance\n","\n","        print('predict state:',(state).shape)\n","        #print('predict state:',(state))\n","        #print('agent.inventory:',agent.inventory)\n","\n","        if model_name == 'DDPG':\n","            actions = agent.act(state, t)\n","            action = np.argmax(actions)\n","        elif model_name in ['ConvLSTM','ConvGRU','LSTM','GRU','RNN']:\n","            actions = agent.model.predict(state.reshape(1,state.shape[1], 1))[0]\n","            action = agent.act(state.reshape(1, state.shape[1], 1))\n","        elif model_name == 'MLP':\n","            actions = agent.model.predict(state.reshape(1,state.shape[1], 1))[0][-1]\n","            action = agent.act(state.reshape(1, state.shape[1], 1))\n","        #elif model_name == 'ConvGRU':\n","        #    actions = agent.model.predict(state.reshape(1,state.shape[1], 1))[0]\n","        #    action = agent.act(state.reshape(1, state.shape[1], 1))\n","        else:\n","            actions = agent.model.predict(state)[0]\n","            action = agent.act(state)\n","        \n","        # execute position\n","        #print(actions)\n","        #print(action)\n","        print('Step: {}\\tHold signal: {:.4} \\tBuy signal: {:.4} \\tSell signal: {:.4}'.format(t, actions[0], actions[1], actions[2]))\n","        #logging.info('Step: {}\\tHold signal: {:.4} \\tBuy signal: {:.4} \\tSell signal: {:.4}'.format(t, actions[0], actions[1], actions[2]))\n","        log_txt(filename,'Step: {}\\tHold signal: {:.4} \\tBuy signal: {:.4} \\tSell signal: {:.4}'.format(t, actions[0], actions[1], actions[2]))\n","        if action != np.argmax(actions): logging.info(f\"\\t\\t'{action_dict[action]}' is an exploration.\")\n","        if action == 0: # hold\n","            execution_result = hold(actions)\n","        if action == 1: # buy\n","            execution_result = buy(t)      \n","        if action == 2: # sell\n","            execution_result = sell(t)        \n","        \n","        print('\\t\\t',action_dict[action],' is an exploration.\\t',stock_name.split(\"_\")[0],model_name,indicator)\n","        # check execution result\n","        if execution_result is None:\n","            reward -= treasury_bond_daily_return_rate() * agent.balance  # missing opportunity\n","        else:\n","            if isinstance(execution_result, tuple): # if execution_result is 'Hold'\n","                actions = execution_result[1]\n","                execution_result = execution_result[0]\n","            #logging.info(execution_result)     \n","            log_txt(filename,execution_result)             \n","        \n","        print('execution_result:',execution_result)\n","        # calculate reward\n","        current_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance\n","        unrealized_profit = current_portfolio_value - agent.initial_portfolio_value\n","        reward += unrealized_profit\n","\n","        agent.portfolio_values.append(current_portfolio_value)\n","        #agent.return_rates.append((current_portfolio_value - previous_portfolio_value) / previous_portfolio_value)\n","        #print('current_portfolio_value:',current_portfolio_value)\n","        #print('previous_portfolio_value:',previous_portfolio_value)\n","        #print('return_rates:',agent.return_rates)\n","        #print('reward:',reward)\n","        #print('agent.portfolio_values:',agent.portfolio_values)\n","        if len(agent.portfolio_values)>1:\n","            agent.return_rates.append((agent.portfolio_values[-1] - agent.portfolio_values[-2]) / agent.portfolio_values[-2])\n","\n","        #print('return_rates:',agent.return_rates)\n","\n","        done = True if t == trading_period else False\n","        agent.remember(state, actions, reward, next_state, done)\n","\n","        # update state\n","        state = next_state\n","\n","        # experience replay\n","        if len(agent.memory) > agent.buffer_size:\n","            num_experience_replay += 1\n","            loss = agent.experience_replay()\n","            aa = agent.balance\n","            bb = agent.inventory\n","            print('Episode: {}\\tLoss: {:.2f}\\tAction: {}\\tReward: {:.2f}\\tBalance: {:.2f}\\tNumber of Stocks: {}\\tindicator: {}'.format(e, loss, action_dict[action], reward, aa, len(bb),indicator))\n","            #logging.info('Episode: {}\\tLoss: {:.2f}\\tAction: {}\\tReward: {:.2f}\\tBalance: {:.2f}\\tNumber of Stocks: {}\\tindicator: {}'.format(e, loss, action_dict[action], reward, aa, len(bb),indicator))\n","            log_txt(filename,'Episode: {}\\tLoss: {:.2f}\\tAction: {}\\tReward: {:.2f}\\tBalance: {:.2f}\\tNumber of Stocks: {}\\tindicator: {}'.format(e, loss, action_dict[action], reward, aa, len(bb),indicator))\n","            agent.tensorboard.on_batch_end(num_experience_replay, {'loss': loss, 'portfolio value': current_portfolio_value})\n","\n","        if done:\n","            portfolio_return = evaluate_portfolio_performance(agent, filename)\n","            returns_across_episodes.append(portfolio_return)\n","\n","    # save models periodically\n","    coinname=stock_name.split(\"_\")[0]\n","    if model_name == 'DDPG':\n","        agent.actor.model.save_weights('./{}/{}_DDPG_w{}_{}_ep{}_actor.h5'.format(model_name,coinname,window_size,indicator,str(e)))\n","        agent.critic.model.save_weights('./{}/{}_DDPG_w{}_{}_ep{}_critic.h5'.format(model_name,coinname,window_size,indicator,str(e)))\n","    else:\n","        agent.model.save('./models/'+model_name+'/'+ coinname + '_' + model_name +'_w' + str(window_size) +'_'+ indicator +'_ep' + str(e) + '.h5')\n","    #logging.info('model saved')\n","    log_txt(filename,'model saved')\n","    \n","#logging.info('total training time: {0:.2f} min'.format((time.time() - start_time)/60))\n","log_txt(filename,'total training time: {0:.2f} min'.format((time.time() - start_time)/60))\n","plot_portfolio_returns_across_episodes(model_name, returns_across_episodes,coinname,indicator)"],"metadata":{"id":"pUoD3AF2MhC1"},"execution_count":null,"outputs":[]}]}